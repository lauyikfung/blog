<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>AI-2 | 刘益枫的博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="干货|卷出顶级AI实验室（2） 进阶 （本人已授权小红书转载） 一、对AI的认知： 深度学习是个很强大的工具，万能近似定理（universal approximation theorem）[Hornik, 1991]理论证明具有隐含层（最少一层）感知机神经网络在激励函数（也称激活函数）为非常数函数的情况下具有逼近任何函数的作用。   然而理论只是证明了其可能性，通常难以得到这种理想的状态，因此对于">
<meta property="og:type" content="article">
<meta property="og:title" content="AI-2">
<meta property="og:url" content="https://lauyikfung.github.io/blog/2024/01/24/%E5%8D%B7%E5%87%BA%E9%A1%B6%E7%BA%A7AI%E5%AE%9E%E9%AA%8C%E5%AE%A4%EF%BC%882%EF%BC%89/index.html">
<meta property="og:site_name" content="刘益枫的博客">
<meta property="og:description" content="干货|卷出顶级AI实验室（2） 进阶 （本人已授权小红书转载） 一、对AI的认知： 深度学习是个很强大的工具，万能近似定理（universal approximation theorem）[Hornik, 1991]理论证明具有隐含层（最少一层）感知机神经网络在激励函数（也称激活函数）为非常数函数的情况下具有逼近任何函数的作用。   然而理论只是证明了其可能性，通常难以得到这种理想的状态，因此对于">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2024-01-24T16:18:00.000Z">
<meta property="article:modified_time" content="2024-02-19T12:40:28.461Z">
<meta property="article:author" content="Lewis Yik-Fung Lau">
<meta property="article:tag" content="刘益枫,枳枫THU, Lewis Yik-Fung Lau">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/blog/atom.xml" title="刘益枫的博客" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/blog/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/blog/css/style.css">

  
    
<link rel="stylesheet" href="/blog/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 6.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/blog/" id="logo">刘益枫的博客</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/blog/" id="subtitle">The Blog of Lewis Yik-Fung Lau</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/blog/">Home</a>
        
          <a class="main-nav-link" href="/blog/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/blog/atom.xml" title="RSS 订阅"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="搜索"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://lauyikfung.github.io/blog"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-卷出顶级AI实验室（2）" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/blog/2024/01/24/%E5%8D%B7%E5%87%BA%E9%A1%B6%E7%BA%A7AI%E5%AE%9E%E9%AA%8C%E5%AE%A4%EF%BC%882%EF%BC%89/" class="article-date">
  <time class="dt-published" datetime="2024-01-24T16:18:00.000Z" itemprop="datePublished">2024-01-25</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      AI-2
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="干货-卷出顶级AI实验室（2）">干货|卷出顶级AI实验室（2）</h2>
<p>进阶</p>
<p>（本人已授权小红书转载）</p>
<p>一、对AI的认知：</p>
<p>深度学习是个很强大的工具，万能近似定理（universal approximation theorem）[Hornik, 1991]理论证明具有隐含层（最少一层）感知机神经网络在激励函数（也称激活函数）为非常数函数的情况下具有逼近任何函数的作用。</p>
<ul>
<li>
<p>然而理论只是证明了其可能性，通常难以得到这种理想的状态，因此对于一般的深度学习模型来说，通常会设计比较复杂的结构，一方面增大其表达能力，从而得到更好的性能；另一方面增大其对参数的容错能力，使其更容易达到较好的性能。</p>
</li>
<li>
<p>从万能近似定理也可以看出来，神经网络一般都由线性结构和激励函数（非线性结构）组成，最简单的神经网络结构就是由激励函数分开的全连接层（多层感知机，MLP）。</p>
</li>
<li>
<p>然而，全连接层参数众多，需要的资源太多、计算速度太慢，因此，将全连接层替换为卷积层，将激励函数变为池化层（maxpool或者avgpool），就变成了最基本的卷积神经网络（CNN），著名的LeNet和AlexNet就是CNN，只不过它们用的是多channel的卷积核。</p>
</li>
<li>
<p>以上的网络都是从前往后依次序的结构，在反向传播的时候容易产生遗忘（梯度消失）的现象，越靠近输入的参数梯度的信号越弱，导致模型容易忽视底层所提取到的特征，例如边缘、纹理等等。因此，ResNet加上一个shortcut连接，即x’=x+F(x)，这样，梯度就能以更快的速度传回前面的模块，从而使得模型能够更好地注意底层特征。而DenseNet进一步推广，在每两个模块之间都加上shortcut连接。</p>
</li>
</ul>
<p>从以上的例子可以看出来，深度学习的研究的一种思路就是看到某个问题需要解决，就设计出一个巧妙的结构去解决它。例如，看到RNN容易导致遗忘问题，就让输入的序列两两之间交互，于是便有了attention。而如果是QA，则需要让输入和输出序列之间、输出序列各token之间都有交互，于是就有了Transformer。</p>
<p>然而，深度学习虽然是万能的工具，但通常需要极其大的数据量去训练（例如GPT4、Kimi Chat等目前较好的大语言模型需要上TB的tokens），而对于一些数据比较少的较简单的任务（比如我之前做的多组学探究只有数百个样本），深度学习往往难以奏效，这时需要考虑一些其它的机器学习算法，例如XGBoost、KNN、SVM等。通常这些算法能带来相当令人满意的精度，其中XGBoost结合SHAP[S. Lundberg&amp;Su-In Lee, 2017]等工具还可以做出比较好的可解释性分析。</p>
<p>对于不同的领域，有不同的研究风格，例如NLP领域，目前LLM就是在Transformer结构上不断进行扩大、修改结构（如pre-norm和post-norm、positional embedding、修改激活函数），而传统NLP则将机器学习算法、数据库等技术融入到BERT等语言模型中，用来实现即插即用、个性化等功能。计算机视觉领域则是在基本CNN上不断推陈出新，如diffusion模型（大家可以想一下为什么diffusion模型用在文本生成任务上表现不尽人意）。另外，对于AI4Science这个方向，因为数据类型和数据量各异，则是要用到多种算法，甚至需要用到一些传统机器学习算法，例如像DNA序列、蛋白质序列等，可以用Transformer（例如Alphafold）；对于像核磁共振等图形，可以用CNN、diffusion等模型；而像组学研究，特征离散且数量巨多，则可以用XGBoost算法。</p>
<p>二、常用工具Pytorch介绍</p>
<p>作为AI相关的研究者，最痛苦的莫过于安装环境。一般会去官网下载安装Anaconda或者Miniconda，而对于AI研究者来说，最重要的包莫过于PyTorch了。</p>
<ul>
<li>所有与PyTorch相关的问题都最好先在https://pytorch.org/上面查询一遍，尤其是查询PyTorch文档（上边栏的Docs下拉菜单里面的PyTorch，直接点搜索符号出来的是Blog）
<ul>
<li>对于英文不太好的uu，可以查找PyTorch中文文档：<a target="_blank" rel="noopener" href="https://pytorch-cn.readthedocs.io/zh/latest/%EF%BC%8C%E4%B8%8D%E8%BF%87%E8%A6%81%E6%83%B3%E6%B7%B1%E5%85%A5%E7%A7%91%E7%A0%94%EF%BC%8C%E8%BF%98%E6%98%AF%E5%BE%97%E6%8F%90%E9%AB%98%E8%8B%B1%E8%AF%AD%E8%83%BD%E5%8A%9B%EF%BC%8C%E5%AD%A6%E4%BC%9A%E9%80%82%E5%BA%94%E8%8B%B1%E6%96%87%E6%96%87%E6%A1%A3">https://pytorch-cn.readthedocs.io/zh/latest/，不过要想深入科研，还是得提高英语能力，学会适应英文文档</a></li>
</ul>
</li>
<li>现在PyTorch的版本已经更新到2.1.x了，理论上兼容1.x的版本，但是会有一些接口上的不同。</li>
<li>安装PyTorch
<ul>
<li>可以到官网https://pytorch.org/get-started/locally/查找对应的安装命令，请注意，官网安装命令默认安装最新版本的PyTorch，实际情况因为算力太高、CUDA版本太低或者python版本较老，可能需要寻找较老版本的PyTorch</li>
<li>安装PyTorch需要先考虑显卡的算力，有些高端显卡（例如GeRorce RTX3090、4090），算力太高，低版本的PyTorch不支持。一些极端情况，高端显卡配上低版本的CUDA，没有合适的PyTorch版本，因此，这种情况需要更新CUDA（总不可能换块显卡吧QAQ）。CUDA版本可以通过nvidia-smi命令查看右上角CUDA处或者nvcc -V查询（但需要注意的是，这两者查到的版本可能是不同的。通常后者版本不大于前者的版本就行）</li>
<li>根据对应的CUDA版本，查询对应的PyTorch适合的版本（<a target="_blank" rel="noopener" href="https://pytorch.org/get-started/previous-versions/%E4%B8%8Acudatoolkit%E7%9A%84%E5%8F%AF%E9%80%89%E8%8C%83%E5%9B%B4%EF%BC%89">https://pytorch.org/get-started/previous-versions/上cudatoolkit的可选范围）</a></li>
<li>有时国内的一些服务器不能访问torch官网，因此安装pytorch可能需要考虑更换源，例如
<ul>
<li>torch==1.9.0 -f <a target="_blank" rel="noopener" href="https://download.pytorch.org/whl/cu110/torch_stable.html">https://download.pytorch.org/whl/cu110/torch_stable.html</a></li>
</ul>
</li>
<li>有些时候因为网络不畅而导致下载很慢，可以手动在https://download.pytorch.org/whl/cu1xx/torch_stable.html（例如https://download.pytorch.org/whl/cu110/torch_stable.html）上面寻找到合适的安装包</li>
<li>用torch.cuda.is_available() （记得先import torch）命令查看你的pytorch版本是不是装正确了（你显卡能不能用）</li>
</ul>
</li>
<li>PyTorch的一些重要包
<ul>
<li>torch.nn.Module和torch.nn.functional
<ul>
<li>前者是一个类，除了要进行对应的计算，还要对模块中的参数等进行管理，而后者只是单纯的计算，需要手动提供对应的参数。</li>
<li>例如nn.Linear包含了一个全连接层，内含了全连接层的weight和bias（如果设定bias=True）参数，而nn.functional.linear需要手动提供weight和bias（默认None）</li>
<li>nn.Module其实是一大堆常见模块，例如n维卷积层、池化层（最大池化Maxpool、平均池化Avgpool等）、非线性层、归一化层（组归一化、批归一化、例归一化）、整合结构（GRU、RNN、LSTM、Transformer（含Encoder、Decoder和单独的layer））、全连接层、dropout、损失函数等等，其中除整合结构外，绝大部分都有对应的nn.functional函数。</li>
<li>一般各种神经网络用到的就是nn.Module下面的各种模块（为了适配nn.sequential()），而nn.functional一般会用于loss的计算以及模块间和最终输出前单独的非线性函数。</li>
<li>这两个再配合torch.view/reshape等函数，就能搭建一个比较复杂的神经网络了</li>
</ul>
</li>
<li>torch.optim
<ul>
<li>这里面包含了一些常见的optimizer，如SGD、Adam、AdamW、RMSprop；以及常用的学习率调节函数（torch.optim.lr_scheduler），如LinearLR，OneCycleLR等等</li>
</ul>
</li>
<li>torch.distributed
<ul>
<li>对于大型项目（如LLM），可能会涉及到多张显卡，因此显卡之间的通讯就非常重要，这里面包含了许多分布式操作函数，例如DDP</li>
</ul>
</li>
<li>torch的梯度机制
<ul>
<li>一般的流程：
<ul>
<li>for epoch in range(num_epoch):</li>
<li>model.train(); for x, target in train_dataset: model.zero_grad(); y = model(x); loss = loss_function(y, target), loss.backward(); optimizer.step(); scheduler.step()</li>
<li>model.eval(); total_loss = 0; for x, target in eval_dataset: model.zero_grad(); y = model(x); total_loss += loss_function(y, target); total_loss = total_loss / len(eval_dataset)</li>
<li>if total_loss &lt; min_loss: torch.save(model)</li>
</ul>
</li>
<li>model.train()和model.eval()：启停模型其中的batch norm和dropout</li>
<li>loss.backward()：传入反向传播命令</li>
<li>optimizer.step()：有时需要手动调用optimizer和scheduler去调整训练梯度的参数和学习率</li>
<li>with torch.no_grad()
<ul>
<li>pytorch有自动求导机制，梯度计算是累加式的，每推算一次就会累加一次梯度相关的参数，因此在evaluation推理期间，需要用这个语句停止自动求导</li>
<li>取消自动求导可以降低存储需要，提高计算速度</li>
</ul>
</li>
</ul>
</li>
<li>torch.nn.init
<ul>
<li>在神经网络初始化时，通常是随机的参数，而用Xavier、Kaiming等初始化策略，可以使得神经网络在初始时就能让反向传播各层方差几乎相等，从而降低梯度收缩和梯度爆炸的可能性。</li>
</ul>
</li>
<li>torch.utils.data.Dataset
<ul>
<li>组织训练、测试和验证数据最常用的类，可以定义批大小、是否随机打乱等等，还支持继承该类进行自定义</li>
</ul>
</li>
</ul>
</li>
<li>PyTorch一些容易被忽略的功能
<ul>
<li>torchsummary
<ul>
<li>可以直接打印函数每一层输出的size，还可以统计每一层的参数数量和总参数量方便写函数的时候debug</li>
<li>用pip install torchsummary
<ul>
<li>from torchsummary import summary</li>
<li>summary(your_model, input_size=(bsz, channels, H, W))，其中input size根据设计模型的输入变化，不一定是4维</li>
</ul>
</li>
</ul>
</li>
<li>torch.einsum
<ul>
<li>对于形象能力强但抽象能力弱的同学，能够形象描述对应的操作，但很难写对应代码的uu，可以试着用torch.einsum</li>
<li>这是基于爱因斯坦求和约定：如果两个相同的指标出现在指标符号公式的同一项中，则表示对该指标遍历整个取值范围求和。</li>
<li>一般形式：torch.einsum(用符号描述你需求的字符串，输入的list)，字符串的格式为：“矩阵1的shape、矩阵2的shape…-&gt;输出矩阵的shape”。</li>
<li>例如：
<ul>
<li>torch.einsum(‘ij, jk-&gt;ik’, [a, b])就是求a与b的矩阵乘法；</li>
<li>torch.einsum(‘jj’, a)就是求矩阵的迹；</li>
<li>torch.einsum(‘…ij-&gt; …ji’, A)就是把A（不管有几维）的最后两维进行转置；</li>
<li>torch.einsum(‘ij-&gt;’, a)就是矩阵求和</li>
<li>torch.einsum(‘ij-&gt;j’, a)就是按列求和</li>
</ul>
</li>
<li>einsum在某些时候（例如二维矩阵点积）可能比对应的函数慢，但有时（例如高维矩阵点乘求和）却可能比相应的函数快</li>
</ul>
</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://lauyikfung.github.io/blog/2024/01/24/%E5%8D%B7%E5%87%BA%E9%A1%B6%E7%BA%A7AI%E5%AE%9E%E9%AA%8C%E5%AE%A4%EF%BC%882%EF%BC%89/" data-id="cm1x270y80009b8vc4etc7ym1" data-title="AI-2" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/blog/2024/02/18/%E6%B6%AA%E9%99%B5%E5%85%AC%E4%BA%A4%E7%BA%BF%E8%B7%AF%E5%9B%BEv2/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">前一篇</strong>
      <div class="article-nav-title">
        
          Bus Route Map
        
      </div>
    </a>
  
  
    <a href="/blog/2024/01/10/%E5%8D%B7%E5%87%BA%E9%A1%B6%E7%BA%A7AI%E5%AE%9E%E9%AA%8C%E5%AE%A4%EF%BC%881%EF%BC%89/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">后一篇</strong>
      <div class="article-nav-title">AI-1</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2024/10/">十月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2024/05/">五月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2024/03/">三月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2024/02/">二月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2024/01/">一月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2022/11/">十一月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2022/10/">十月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2022/08/">八月 2022</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/blog/2024/10/05/CS495/">CS495 of UCLA</a>
          </li>
        
          <li>
            <a href="/blog/2024/05/18/AIandUs/">AI, Large Language Models, and Us</a>
          </li>
        
          <li>
            <a href="/blog/2024/03/30/Yao-Seminar/">Yao-Seminar</a>
          </li>
        
          <li>
            <a href="/blog/2024/02/18/%E6%B6%AA%E9%99%B5%E5%85%AC%E4%BA%A4%E7%BA%BF%E8%B7%AF%E5%9B%BEv2/">Bus Route Map</a>
          </li>
        
          <li>
            <a href="/blog/2024/01/24/%E5%8D%B7%E5%87%BA%E9%A1%B6%E7%BA%A7AI%E5%AE%9E%E9%AA%8C%E5%AE%A4%EF%BC%882%EF%BC%89/">AI-2</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 Lewis Yik-Fung Lau<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/blog/" class="mobile-nav-link">Home</a>
  
    <a href="/blog/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/blog/js/jquery-3.4.1.min.js"></script>



  
<script src="/blog/fancybox/jquery.fancybox.min.js"></script>




<script src="/blog/js/script.js"></script>





  </div>
</body>
</html>